{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kSNhSuPjqvgd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow==2.10.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (2.10.0)"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (3.10.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (23.2)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.0)\n","  Using cached protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n","Requirement already satisfied: setuptools in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (68.0.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (4.7.1)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.16.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (0.31.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (1.59.3)\n","Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (2.10.1)\n","Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (2.10.0)\n","Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow==2.10.0) (2.10.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.23.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.1)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.17.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n","Installing collected packages: protobuf\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","Successfully installed protobuf-3.19.6\n","Requirement already satisfied: tensorflow_datasets in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (4.9.3)\n","Requirement already satisfied: absl-py in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (1.4.0)\n","Requirement already satisfied: array-record in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (0.4.1)\n","Requirement already satisfied: click in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n","Requirement already satisfied: dm-tree in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (0.1.8)\n","Requirement already satisfied: etils>=0.9.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.5.2)\n","Requirement already satisfied: numpy in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (1.25.2)\n","Requirement already satisfied: promise in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (2.3)\n","Collecting protobuf>=3.20 (from tensorflow_datasets)\n","  Using cached protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n","Requirement already satisfied: psutil in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (5.9.0)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (2.31.0)\n","Requirement already satisfied: tensorflow-metadata in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (1.14.0)\n","Requirement already satisfied: termcolor in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (2.3.0)\n","Requirement already satisfied: toml in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n","Requirement already satisfied: tqdm in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (4.66.1)\n","Requirement already satisfied: wrapt in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow_datasets) (1.16.0)\n","Requirement already satisfied: fsspec in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.10.0)\n","Requirement already satisfied: importlib_resources in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.1.1)\n","Requirement already satisfied: typing_extensions in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.7.1)\n","Requirement already satisfied: zipp in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.11.17)\n","Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n","Requirement already satisfied: six in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.61.0)\n","  Using cached protobuf-3.20.3-cp39-cp39-win_amd64.whl (904 kB)\n","Installing collected packages: protobuf\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.19.6\n","    Uninstalling protobuf-3.19.6:\n","      Successfully uninstalled protobuf-3.19.6\n","Successfully installed protobuf-3.20.3\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n","tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: matplotlib in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (3.7.3)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (4.45.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (10.0.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from matplotlib) (6.1.1)\n","Requirement already satisfied: zipp>=3.1.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: pandas in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.20.3 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer==3.1.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (3.1.0)\n","Requirement already satisfied: tensorflow-addons in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (0.22.0)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n","Requirement already satisfied: packaging in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tensorflow-addons) (23.2)\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","c:\\Users\\Administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n"]}],"source":["from textwrap import wrap\n","import os\n","\n","!pip install keras-cv==0.4.0 -q\n","#!pip install git+https://github.com/keras-team/keras-cv.git tensorflow --upgrade\n","!pip install tensorflow==2.10.0\n","!pip install tensorflow_datasets\n","!pip install matplotlib\n","!pip install pandas\n","!pip install charset-normalizer==3.1.0\n","!pip install -U tensorflow-addons\n","import tensorflow_addons as tfa\n","\n","import keras_cv\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.experimental.numpy as tnp\n","from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n","from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n","from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n","from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n","from keras_cv.models.stable_diffusion.text_encoder import TextEncoder\n","from tensorflow import keras\n","tf.keras.backend.set_floatx('float32')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  2\n"]}],"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tHpyR0RyyGGL"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>caption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>D:/dataset\\100023.jpg</td>\n","      <td>Nước ngọt CocaCola</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>D:/dataset\\100023.jpg</td>\n","      <td>Thức uống có gas</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>D:/dataset\\100033.jpg</td>\n","      <td>Chiếc xe ô tô màu trắng</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>D:/dataset\\100033.jpg</td>\n","      <td>Xe ô tô</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>D:/dataset\\100113.jpg</td>\n","      <td>Món ăn sushi của Nhật Bản</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              image_path                    caption\n","0  D:/dataset\\100023.jpg         Nước ngọt CocaCola\n","1  D:/dataset\\100023.jpg           Thức uống có gas\n","2  D:/dataset\\100033.jpg    Chiếc xe ô tô màu trắng\n","3  D:/dataset\\100033.jpg                    Xe ô tô\n","4  D:/dataset\\100113.jpg  Món ăn sushi của Nhật Bản"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data_path = 'D:/dataset'\n","\n","data_frame = pd.read_csv(os.path.join(data_path, \"data.csv\"))\n","\n","data_frame[\"image_path\"] = data_frame[\"image_path\"].apply(\n","    lambda x: os.path.join(data_path, x)\n",")\n","data_frame.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"PGTAkB_lIgVW"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (4.35.2)\n","Requirement already satisfied: filelock in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (2023.10.3)\n","Requirement already satisfied: requests in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n","Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: tokenizers in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (0.15.0)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tokenizers) (0.19.4)\n","Requirement already satisfied: filelock in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.10.0)\n","Requirement already satisfied: requests in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n","Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\envs\\sd_keras\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n"]}],"source":["!pip install transformers\n","!pip3 install tokenizers"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":964,"status":"ok","timestamp":1699375075963,"user":{"displayName":"Khương Huỳnh Gia","userId":"00717206766545059244"},"user_tz":-420},"id":"DJlnr5rjyMY0"},"outputs":[],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer\n","PADDING_TOKEN = 64000\n","MAX_PROMPT_LENGTH = 256\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\", add_special_tokens=False)\n","\n","def process_text(caption):\n","    tokens = tokenizer.encode(caption)\n","    tokens = torch.tensor([tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))])\n","    return tokens\n","\n","# Collate the tokenized captions into an array.\n","tokenized_texts = np.empty((len(data_frame), MAX_PROMPT_LENGTH))\n","\n","all_captions = list(data_frame[\"caption\"].values)\n","\n","for i, caption in enumerate(all_captions):\n","    tokenized_texts[i] = process_text(caption)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3104,"status":"ok","timestamp":1699375101506,"user":{"displayName":"Khương Huỳnh Gia","userId":"00717206766545059244"},"user_tz":-420},"id":"yVQKlbdwyjQF","outputId":"c070c0c5-78c6-4f93-cfcf-5e929f403a41"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["RESOLUTION = 256\n","AUTO = tf.data.AUTOTUNE\n","#POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int64)\n","\n","augmenter = keras.Sequential(\n","    layers=[\n","        keras_cv.layers.CenterCrop(RESOLUTION, RESOLUTION),\n","        #keras_cv.layers.RandomFlip(),\n","        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n","    ]\n",")\n","text_encoder = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n","# def process_image(image_path, tokenized_text):\n","#     image = tf.io.read_file(image_path)\n","#     image = tf.io.decode_png(image, 3)\n","#     image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n","#     return image, tokenized_text\n","\n","def process_image(image_path):\n","    image = tf.io.read_file(image_path)\n","    image = tf.io.decode_png(image, 3)\n","    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n","    return image\n","\n","# def apply_augmentation(image_batch, token_batch):\n","#     return augmenter(image_batch), token_batch\n","\n","def apply_augmentation(image_batch):\n","    return augmenter(image_batch)\n","\n","def run_text_encoder(image_batch, token_batch):\n","    return (\n","        image_batch,\n","        token_batch,\n","        text_encoder(token_batch),\n","    )\n","\n","\n","def prepare_dict(image_batch, token_batch, encoded_text_batch):\n","    return {\n","        \"images\": image_batch,\n","        \"tokens\": token_batch,\n","        \"encoded_text\": encoded_text_batch,\n","    }\n","\n","def prepare_dataset(image_paths, tokenized_texts, batch_size=1):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts.astype(np.int32)))\n","    dataset = dataset.shuffle(batch_size * 10)\n","    # dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)\n","    # dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)\n","    # dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)\n","    # dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)\n","    return dataset.prefetch(AUTO)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20873,"status":"ok","timestamp":1699375125169,"user":{"displayName":"Khương Huỳnh Gia","userId":"00717206766545059244"},"user_tz":-420},"id":"YtUIUsQVywFb","outputId":"f41635f4-8aec-41f8-f9e7-8261d7d47032"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_3776\\532460767.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_2112s1s0to\\croot\\pytorch-select_1700158736573\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n","  tf_element = torch.tensor([np.array(element[1].numpy())])\n"]}],"source":["with tf.device('/CPU:0'):\n","    # Prepare the dataset.\n","    BATCH_SIZE = 4\n","    tf_training_dataset = prepare_dataset(\n","        np.array(data_frame[\"image_path\"]), tokenized_texts, batch_size=4\n","    )\n","    list_dict = []\n","    for element in tf_training_dataset:\n","        image = process_image(element[0].numpy())\n","        image_augmentation = apply_augmentation(image)\n","        tf_element = torch.tensor([np.array(element[1].numpy())])\n","        encoded_text_batch = text_encoder(tf_element)\n","        dict_dt = prepare_dict(image_augmentation, element[1], tf.convert_to_tensor(encoded_text_batch.last_hidden_state[0].detach().numpy()))\n","        list_dict.append(dict_dt)\n","\n","    training_dataset = tf.data.Dataset.from_tensor_slices(pd.DataFrame.from_dict(list_dict).to_dict(orient=\"list\"))\n","    training_dataset = training_dataset.shuffle(BATCH_SIZE * 10)\n","    training_dataset = training_dataset.batch(BATCH_SIZE)\n","\n","    # Take a sample batch and investigate.\n","    sample_batch = next(iter(training_dataset))\n","\n","    for k in sample_batch:\n","        print(k, sample_batch[k].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mFwIRwyy4vd"},"outputs":[],"source":["class Trainer(tf.keras.Model):\n","    # Reference:\n","    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\n","\n","    def __init__(\n","        self,\n","        diffusion_model,\n","        vae,\n","        noise_scheduler,\n","        use_mixed_precision=False,\n","        max_grad_norm=1.0,\n","        **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","\n","        self.diffusion_model = diffusion_model\n","        self.vae = vae\n","        self.noise_scheduler = noise_scheduler\n","        self.max_grad_norm = max_grad_norm\n","\n","        self.use_mixed_precision = use_mixed_precision\n","        self.vae.trainable = False\n","\n","    def train_step(self, inputs):\n","        images = inputs[\"images\"]\n","        encoded_text = inputs[\"encoded_text\"]\n","        batch_size = tf.shape(images)[0]\n","\n","        with tf.GradientTape() as tape:\n","            # Project image into the latent space and sample from it.\n","            latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n","            # Know more about the magic number here:\n","            # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n","            latents = latents * 0.18215\n","\n","            # Sample noise that we'll add to the latents.\n","            noise = tf.random.normal(tf.shape(latents))\n","\n","            # Sample a random timestep for each image.\n","            timesteps = tnp.random.randint(\n","                0, self.noise_scheduler.train_timesteps, (batch_size,)\n","            )\n","\n","            # Add noise to the latents according to the noise magnitude at each timestep\n","            # (this is the forward diffusion process).\n","            noisy_latents = self.noise_scheduler.add_noise(\n","                tf.cast(latents, dtype=tf.float32), tf.cast(noise, dtype=tf.float32), timesteps\n","            )\n","\n","            # Get the target for loss depending on the prediction type\n","            # just the sampled noise for now.\n","            target = noise  # noise_schedule.predict_epsilon == True\n","\n","            # Predict the noise residual and compute loss.\n","            timestep_embedding = tf.map_fn(\n","                lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n","            )\n","            timestep_embedding = tf.squeeze(timestep_embedding, 1)\n","            model_pred = self.diffusion_model(\n","                [noisy_latents, timestep_embedding, encoded_text], training=True\n","            )\n","            loss = self.compiled_loss(target, model_pred)\n","            if self.use_mixed_precision:\n","                loss = self.optimizer.get_scaled_loss(loss)\n","\n","        # Update parameters of the diffusion model.\n","        trainable_vars = self.diffusion_model.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        if self.use_mixed_precision:\n","            gradients = self.optimizer.get_unscaled_gradients(gradients)\n","        gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n","        half = dim // 2\n","        log_max_period = tf.math.log(tf.cast(max_period, tf.float32))\n","        freqs = tf.math.exp(\n","            -log_max_period * tf.range(0, half, dtype=tf.float32) / half\n","        )\n","        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n","        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n","        embedding = tf.reshape(embedding, [1, -1])\n","        return embedding\n","\n","    def sample_from_encoder_outputs(self, outputs):\n","        mean, logvar = tf.split(outputs, 2, axis=-1)\n","        logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n","        std = tf.exp(0.5 * logvar)\n","        sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n","        return mean + std * sample\n","\n","    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n","        # Overriding this method will allow us to use the `ModelCheckpoint`\n","        # callback directly with this trainer class. In this case, it will\n","        # only checkpoint the `diffusion_model` since that's what we're training\n","        # during fine-tuning.\n","        self.diffusion_model.save_weights(\n","            filepath=filepath,\n","            overwrite=overwrite,\n","            save_format=save_format,\n","            options=options,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2lfpfdzy8oU"},"outputs":[],"source":["# Enable mixed-precision training if the underlying GPU has tensor cores.\n","with tf.device('/GPU:0'):\n","    USE_MP = True\n","    if USE_MP:\n","        keras.mixed_precision.set_global_policy(\"float32\")\n","\n","    image_encoder = ImageEncoder(RESOLUTION, RESOLUTION, download_weights=False)\n","    diffusion_ft_trainer = Trainer(\n","        diffusion_model=DiffusionModel(RESOLUTION, RESOLUTION, MAX_PROMPT_LENGTH, download_weights=False),\n","        # Remove the top layer from the encoder, which cuts off the variance and only\n","        # returns the mean.\n","        vae=tf.keras.Model(\n","            image_encoder.input,\n","            image_encoder.layers[-2].output,\n","        ),\n","        noise_scheduler=NoiseScheduler(),\n","        use_mixed_precision=USE_MP,\n","    )\n","\n","    # These hyperparameters come from this tutorial by Hugging Face:\n","    # https://huggingface.co/docs/diffusers/training/text2image\n","    lr = 1e-5\n","    beta_1, beta_2 = 0.9, 0.999\n","    weight_decay = 1e-2\n","    epsilon = 1e-08\n","\n","    optimizer = tf.keras.optimizers.experimental.AdamW(\n","        learning_rate=lr,\n","        weight_decay=weight_decay,\n","        beta_1=beta_1,\n","        beta_2=beta_2,\n","        epsilon=epsilon,\n","    )\n","    #optimizer_s = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n","    diffusion_ft_trainer.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BpEg5kqJFsD"},"outputs":[],"source":["from tqdm.keras import TqdmCallback\n","with tf.device('/GPU:1'):\n","    epochs = 20\n","    ckpt_path = \"finetuned_stable_diffusion.h5\"\n","    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n","        ckpt_path,\n","        save_weights_only=True,\n","        monitor=\"loss\",\n","        mode=\"min\",\n","        save_best_only=False,\n","        save_freq='epoch',\n","    )\n","    diffusion_ft_trainer.fit(training_dataset, epochs=epochs, verbose=0, callbacks=[ckpt_callback,TqdmCallback(verbose=1)])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOoaOaA1yjLxQP/sNoVPq3Z","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
